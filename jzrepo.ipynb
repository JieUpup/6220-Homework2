{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JieUpup/6220-Homework2/blob/main/jzrepo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KZGt_J3yT7b",
        "outputId": "6c57b2a1-c3f4-48c2-caa1-51d143b468aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=184c8e905bc0417ade72e2c81fff5d7d972157699fea3a293b5ddcbe490ea6d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-510\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 2 newly installed, 0 to remove and 27 not upgraded.\n",
            "Need to get 36.5 MB of archives.\n",
            "After this operation, 143 MB of additional disk space will be used.\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "(Reading database ... 129496 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u352-ga-1~20.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u352-ga-1~20.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u352-ga-1~20.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u352-ga-1~20.04) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u352-ga-1~20.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u352-ga-1~20.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gHd7DgMy6FY"
      },
      "outputs": [],
      "source": [
        "#@title Create a Spark Session and Context\n",
        "\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext\n",
        "import pandas as pd\n",
        "\n",
        "# create the Spark Session\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# create the Spark Context\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZBcdxz7zBb9"
      },
      "outputs": [],
      "source": [
        "path = \"soc-LiveJournal1Adj.txt\"\n",
        "df1 = spark.read.text(path)\n",
        "df1.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_data = open(\"small_sample.txt\", \"w\")  # append mode\n",
        "sample_data.write(\"1\\t2,3,5\\n\")\n",
        "sample_data.write(\"2\\t1,3,5,6\\n\")\n",
        "sample_data.write(\"3\\t1,2,7\\n\")\n",
        "sample_data.write(\"4\\t5,6,7\\n\")\n",
        "sample_data.write(\"5\\t1,2,4\\n\")\n",
        "sample_data.write(\"6\\t2,4,7\\n\")\n",
        "sample_data.write(\"7\\t3,4,6\\n\")\n",
        "sample_data.close()\n",
        "\n",
        "df1 = spark.read.text(\"small_sample.txt\")\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeKUIqJX8lQC",
        "outputId": "c9cd1433-e51e-4dcf-e163-3fba6f1519dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|     value|\n",
            "+----------+\n",
            "|  1\\t2,3,5|\n",
            "|2\\t1,3,5,6|\n",
            "|  3\\t1,2,7|\n",
            "|  4\\t5,6,7|\n",
            "|  5\\t1,2,4|\n",
            "|  6\\t2,4,7|\n",
            "|  7\\t3,4,6|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vfd32cEs0zEQ",
        "outputId": "e1b75bff-103f-457a-d8b2-7864c77792f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "|_c0|    _c1|\n",
            "+---+-------+\n",
            "|  1|  2,3,5|\n",
            "|  2|1,3,5,6|\n",
            "|  3|  1,2,7|\n",
            "|  4|  5,6,7|\n",
            "|  5|  1,2,4|\n",
            "|  6|  2,4,7|\n",
            "|  7|  3,4,6|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df2 = spark.read.option(\"delimiter\", \"\\t\").csv(\"small_sample.txt\")\n",
        "df2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsVFBNWh1INK",
        "outputId": "0bd4cf75-9dd6-4e8a-fefa-d24604fe46fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+\n",
            "|user_id|friends|\n",
            "+-------+-------+\n",
            "|      1|  2,3,5|\n",
            "|      2|1,3,5,6|\n",
            "|      3|  1,2,7|\n",
            "|      4|  5,6,7|\n",
            "|      5|  1,2,4|\n",
            "|      6|  2,4,7|\n",
            "|      7|  3,4,6|\n",
            "+-------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# formated out put, show the relationship in user_id with diffrent id\n",
        "df2 = df2.withColumnRenamed('_c0', 'user_id')\n",
        "df2 = df2.withColumnRenamed('_c1', 'friends')\n",
        "df2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqaq9IAM4eDz",
        "outputId": "e2a111f8-7d58-45ae-ba3d-d3f7fa10e5c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "# define a function to get the main_id \n",
        "def get_main_id(df, main_id):\n",
        "    df = df.filter(df.user_id == str(main_id))\n",
        "    r1 = df.select('friends').collect()[0]\n",
        "\n",
        "    return (str(main_id), r1.__getitem__('friends'))\n",
        "\n",
        "# define a function to find the common friends between two lists, use cur_id\n",
        "# to track the iterator\n",
        "def common(main_id, cur_id, f1, f2):\n",
        "  if main_id == cur_id:\n",
        "    return 0\n",
        "  # use two arraies to store the result, check the main_id exit in arr2, \n",
        "  # otherwise, sort two array and keep track by the index and \n",
        "  # using a counter to count\n",
        "  arr1 = f1.split(',') if f1 != None else []\n",
        "  arr2 = f2.split(',') if f2 != None else []\n",
        "  if main_id in arr2:\n",
        "    return 0\n",
        "  else:\n",
        "    arr1.sort()\n",
        "    arr2.sort()\n",
        "    index1 = 0\n",
        "    index2 = 0\n",
        "    count = 0\n",
        "    while index1 < len(arr1) and index2 < len(arr2):\n",
        "      if arr1[index1] == arr2[index2]:\n",
        "        count += 1\n",
        "        index1 += 1\n",
        "        index2 += 1\n",
        "      elif arr1[index1] > arr2[index2]:\n",
        "        index2 += 1\n",
        "      else:\n",
        "        index1 += 1\n",
        "    return count\n",
        "\n",
        "main_id = '2'\n",
        "main_id, main_friends = get_main_id(df2, main_id)\n",
        "print(common(main_id, '2', main_friends, '3,4,5'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TW_bp4I87WIQ"
      },
      "outputs": [],
      "source": [
        "# define a function called top_10 to calculate the top 10 recommendation \n",
        "def top_10(df, main_id, main_friends):\n",
        "  # map and filter\n",
        "  df_tu = df.rdd.map(lambda f : (int(f[0]), common(main_id, f[0], main_friends, f[1]))) \\\n",
        "      .filter(lambda f : f[1] > 0)\n",
        "  if df_tu.isEmpty():\n",
        "    return \"\"\n",
        "  df2 = df_tu.toDF(['user_id', 'common_friends'])\n",
        "  # create a top_10_list, sort\n",
        "  top_10_list = df2.sort(desc('common_friends'), asc('user_id')).select(col('user_id')).take(10)\n",
        "  top_10_list = [str(e.__getitem__('user_id')) for e in top_10_list]\n",
        "  # add top_10_list to the result\n",
        "  friend_suggestions = \",\".join(top_10_list)\n",
        "\n",
        "  return friend_suggestions\n",
        "\n",
        "#new_rdd = top_10(df2, '11')\n",
        "#print(new_rdd)\n",
        "# read 1000 data for demo\n",
        "li = df2.rdd.take(1000)\n",
        "\n",
        "#df2_ddl = sc.parallelize(li)\n",
        "#df3_ddl = df2.rdd.map(lambda x : (x[0], top_10(df2, x[0], x[1])))\n",
        "#print(df3_ddl)\n",
        "#print(type(li))\n",
        "\n",
        "output = []\n",
        "for user in li:\n",
        "  user_id = user['user_id']\n",
        "  friends = user['friends']\n",
        "  suggestions = top_10(df2, user_id, friends)\n",
        "  output.append( str(user_id) + '\\t' + suggestions )\n",
        "\n",
        "\n",
        "with open('output.text', 'w') as txt_file:\n",
        "  for line in output:\n",
        "    txt_file.write(line + '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!more output.text"
      ],
      "metadata": {
        "id": "DaFseLa883pK",
        "outputId": "246a8d3f-f6cc-4fec-8378-de76ce939bb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "more: stat of output.text failed: No such file or directory\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}