{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JieUpup/6220-Homework2/blob/main/jzrepo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KZGt_J3yT7b",
        "outputId": "3e37f6d9-edec-4ea2-fda6-db7f1a74f1b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=47324e247eed8960f112ba909a24e9b68e689427e1c4c3ee718e1b6641911f7c\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-510\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra\n",
            "  fonts-ipafont-gothic fonts-ipafont-mincho fonts-wqy-microhei\n",
            "  fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 2 newly installed, 0 to remove and 21 not upgraded.\n",
            "Need to get 36.5 MB of archives.\n",
            "After this operation, 143 MB of additional disk space will be used.\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "(Reading database ... 128048 files and directories currently installed.)\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u352-ga-1~20.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u352-ga-1~20.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u352-ga-1~20.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u352-ga-1~20.04) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u352-ga-1~20.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u352-ga-1~20.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7gHd7DgMy6FY"
      },
      "outputs": [],
      "source": [
        "#@title Create a Spark Session and Context\n",
        "\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext\n",
        "import pandas as pd\n",
        "\n",
        "# create the Spark Session\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# create the Spark Context\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nZBcdxz7zBb9",
        "outputId": "66f12718-4fbf-4a2d-850f-37323a20fb0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-fe8e2f212383>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"soc-LiveJournal1Adj.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mtext\u001b[0;34m(self, paths, wholetext, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter)\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     def csv(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/content/soc-LiveJournal1Adj.txt"
          ]
        }
      ],
      "source": [
        "path = \"soc-LiveJournal1Adj.txt\"\n",
        "df1 = spark.read.text(path)\n",
        "df1.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# small data demo and display\n",
        "sample_data = open(\"small_sample.txt\", \"w\")  # append mode\n",
        "sample_data.write(\"1\\t2,3,5\\n\")\n",
        "sample_data.write(\"2\\t1,3,5,6\\n\")\n",
        "sample_data.write(\"3\\t1,2,7\\n\")\n",
        "sample_data.write(\"4\\t5,6,7\\n\")\n",
        "sample_data.write(\"5\\t1,2,4\\n\")\n",
        "sample_data.write(\"6\\t2,4,7\\n\")\n",
        "sample_data.write(\"7\\t3,4,6\\n\")\n",
        "sample_data.close()\n",
        "\n",
        "df1 = spark.read.text(\"small_sample.txt\")\n",
        "df1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeKUIqJX8lQC",
        "outputId": "a2d5e031-cdd0-42fb-8752-3a5ea2eba0a3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|     value|\n",
            "+----------+\n",
            "|  1\\t2,3,5|\n",
            "|2\\t1,3,5,6|\n",
            "|  3\\t1,2,7|\n",
            "|  4\\t5,6,7|\n",
            "|  5\\t1,2,4|\n",
            "|  6\\t2,4,7|\n",
            "|  7\\t3,4,6|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vfd32cEs0zEQ",
        "outputId": "e11ca1c0-9179-4222-d685-c35b6a4e5f7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "|_c0|    _c1|\n",
            "+---+-------+\n",
            "|  1|  2,3,5|\n",
            "|  2|1,3,5,6|\n",
            "|  3|  1,2,7|\n",
            "|  4|  5,6,7|\n",
            "|  5|  1,2,4|\n",
            "|  6|  2,4,7|\n",
            "|  7|  3,4,6|\n",
            "+---+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# formart the data frame\n",
        "df2 = spark.read.option(\"delimiter\", \"\\t\").csv(\"small_sample.txt\")\n",
        "df2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsVFBNWh1INK",
        "outputId": "2c133f90-162f-49bd-f17b-af39c2486cd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+\n",
            "|user_id|friends|\n",
            "+-------+-------+\n",
            "|      1|  2,3,5|\n",
            "|      2|1,3,5,6|\n",
            "|      3|  1,2,7|\n",
            "|      4|  5,6,7|\n",
            "|      5|  1,2,4|\n",
            "|      6|  2,4,7|\n",
            "|      7|  3,4,6|\n",
            "+-------+-------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df2 = df2.withColumnRenamed('_c0', 'user_id')\n",
        "df2 = df2.withColumnRenamed('_c1', 'friends')\n",
        "df2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqaq9IAM4eDz",
        "outputId": "304bd483-b53b-438f-e3b4-86e417363cd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "# define a function to get the id by \"friends\"\n",
        "def get_main_id(df, main_id):\n",
        "    df = df.filter(df.user_id == str(main_id))\n",
        "    r1 = df.select('friends').collect()[0]\n",
        "\n",
        "    return (str(main_id), r1.__getitem__('friends'))\n",
        "\n",
        "# define a fucntion to find the common friends, use two parameters f1 and f2\n",
        "\n",
        "def common(main_id, cur_id, f1, f2):\n",
        "  # use current_id to track the main_id\n",
        "  if main_id == cur_id:\n",
        "    return 0\n",
        "\n",
        "  arr1 = f1.split(',') if f1 != None else []\n",
        "  arr2 = f2.split(',') if f2 != None else []\n",
        "  if main_id in arr2:\n",
        "    return 0\n",
        "  else:\n",
        "    # use sort and track the index of array1 and array2, use viriable count\n",
        "    # to update the interator\n",
        "    arr1.sort()\n",
        "    arr2.sort()\n",
        "    index1 = 0\n",
        "    index2 = 0\n",
        "    count = 0\n",
        "    while index1 < len(arr1) and index2 < len(arr2):\n",
        "      if arr1[index1] == arr2[index2]:\n",
        "        count += 1\n",
        "        index1 += 1\n",
        "        index2 += 1\n",
        "      elif arr1[index1] > arr2[index2]:\n",
        "        index2 += 1\n",
        "      else:\n",
        "        index1 += 1\n",
        "    return count\n",
        "\n",
        "main_id = '2'\n",
        "main_id, main_friends = get_main_id(df2, main_id)\n",
        "print(common(main_id, '2', main_friends, '3,4,5'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TW_bp4I87WIQ"
      },
      "outputs": [],
      "source": [
        "# define top_10 recomendation for user id.\n",
        "def top_10(df, main_id, main_friends):\n",
        "  # use inline lamda functon to mapping the result and filter.\n",
        "  df_tu = df.rdd.map(lambda f : (int(f[0]), common(main_id, f[0], main_friends, f[1]))) \\\n",
        "      .filter(lambda f : f[1] > 0)\n",
        "  if df_tu.isEmpty():\n",
        "    return \"\"\n",
        "  df2 = df_tu.toDF(['user_id', 'common_friends'])\n",
        "  top_10_list = df2.sort(desc('common_friends'), asc('user_id')).select(col('user_id')).take(10)\n",
        "  # use getitem method by user id and make sure it already exist in the previous list\n",
        "  top_10_list = [str(e.__getitem__('user_id')) for e in top_10_list]\n",
        "  friend_suggestions = \",\".join(top_10_list)\n",
        "\n",
        "  return friend_suggestions\n",
        "\n",
        "#new_rdd = top_10(df2, '11')\n",
        "#print(new_rdd)\n",
        "\n",
        "li = df2.rdd.take(1000)\n",
        "\n",
        "#df2_ddl = sc.parallelize(li)\n",
        "#df3_ddl = df2.rdd.map(lambda x : (x[0], top_10(df2, x[0], x[1])))\n",
        "#print(df3_ddl)\n",
        "#print(type(li))\n",
        "\n",
        "output = []\n",
        "for user in li:\n",
        "  user_id = user['user_id']\n",
        "  friends = user['friends']\n",
        "  suggestions = top_10(df2, user_id, friends)\n",
        "  output.append( str(user_id) + '\\t' + suggestions )\n",
        "\n",
        "\n",
        "with open('output.text', 'w') as txt_file:\n",
        "  for line in output:\n",
        "    txt_file.write(line + '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!more output.text"
      ],
      "metadata": {
        "id": "DaFseLa883pK",
        "outputId": "f6e44bc2-3acc-46c7-faa7-4b655590ec32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\t4,6,7\n",
            "2\t4,7\n",
            "3\t5,6,4\n",
            "4\t2,1,3\n",
            "5\t3,6,7\n",
            "6\t3,5,1\n",
            "7\t2,1,5\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}